{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ML pipeline Using the AWS Glue Workflow\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Build a Machine Learning Workflow](#Build-a-Machine-Learning-Workflow)\n",
    "1. [Run the Workflow](#Run-the-Workflow)\n",
    "1. [Evaluate the deployed model](#Evaluate-the-deployed-model)\n",
    "1. [Clean Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook describes how to use Glue Workflow with PySpark scripts to create a machine learning pipeline across data preparation, model training, model evaluation and model register. The defintion of workflow as beflow:\n",
    "\n",
    "<div align=\"center\"><img width=600 src=\"images/glue_workflow_pipeline.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM Permission and Role\n",
    "\n",
    "* Required IAM roles on services.\n",
    "\n",
    "To execute the notebook and Glue Workflow, we will need to manage access control for services.\n",
    "\n",
    "  * IAM role for SageMaker (Studio) Notebook - the execution role configuration\n",
    "    * Open the Amazon [SageMaker console](https://console.aws.amazon.com/sagemaker/). \n",
    "    * Get the SageMaker execution role from console (via opening SageMaker Notebook Instance detail or opening user profile detail under SageMaker Studio domain)\n",
    "    * Open the SageMaker execution role from IAM, and attached below managed IAM policy for it:\n",
    "        * arn:aws:iam::aws:policy/AWSGlueConsoleSageMakerNotebookFullAccess\n",
    "                            \n",
    "  * IAM role for Glue job to execute data access from S3 and model training on SageMaker\n",
    "    * With executing a script to create role `AWS-Glue-S3-SageMaker-Access` below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "\n",
    "sys.path.insert( 0, os.path.abspath(\"./code\") )\n",
    "import setup_iam_roles\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "id = uuid.uuid4().hex\n",
    "\n",
    "# SageMaker Execution Role\n",
    "sagemaker_execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Create a unique name for the AWS Glue job to be created. If you change the\n",
    "# default name, you may need to change the Step Functions execution role.\n",
    "glue_job_prefix = \"customer-churn-etl\"\n",
    "glue_job_name = f\"{glue_job_prefix}-{id}\"\n",
    "\n",
    "# Create a unique name for the AWS Lambda function to be created. If you change\n",
    "# the default name, you may need to change the Step Functions execution role.\n",
    "query_function_prefix = \"query-evaluation-result\"\n",
    "query_function_name = f\"{query_function_prefix}-{id}\"\n",
    "\n",
    "# endpoint name\n",
    "current_time = datetime.now()\n",
    "timestamp_suffix = str(current_time.month) + \"-\" + str(current_time.day) + \"-\" + str(current_time.hour) + \"-\" + str(current_time.minute)\n",
    "\n",
    "endpoint_name = f\"gw-customer-churn-endpoint-{timestamp_suffix}\"\n",
    "\n",
    "prefix = 'sagemaker/DEMO-xgboost-customer-churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an IAM role for Glue Job\n",
    "* Providing access on the S3 bucket\n",
    "* Executing SageMaker training job and model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ARN from existing role: AWS-Glue-S3-SageMaker-Access\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::452533547478:role/AWS-Glue-S3-SageMaker-Access'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_role_name = \"AWS-Glue-S3-SageMaker-Access\"\n",
    "glue_role_arn = setup_iam_roles.create_glue_role(glue_role_name, bucket)\n",
    "glue_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataset\n",
    "This notebook uses the XGBoost algorithm to automate the classification of unhappy customers for telecommunication service providers. The goal is to identify customers who may cancel their service soon so that you can entice them to stay. This is known as customer churn prediction.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prefix = \"train\"\n",
    "val_prefix = \"validation\"\n",
    "test_prefix = \"test\"\n",
    "\n",
    "raw_data = f\"s3://{bucket}/{prefix}/input\"\n",
    "batch_transform_output = f\"s3://{bucket}/{prefix}/batch_transform\"\n",
    "processed_data = f\"s3://{bucket}/{prefix}/processed\"\n",
    "\n",
    "train_data = f\"{processed_data}/{train_prefix}/\"\n",
    "validation_data = f\"{processed_data}/{val_prefix}/\"\n",
    "test_data = f\"{processed_data}/{test_prefix}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to `S3 Bucket`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-southeast-2-452533547478/sagemaker/DEMO-xgboost-customer-churn/input/churn_processed.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3Uploader.upload(\n",
    "    local_path=\"../data/churn_processed.csv\",\n",
    "    desired_s3_uri=f\"{raw_data}\",\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Glue Workflow as the orchestration engine, Glue Job for the data preprocessing and model training/deployment as the steps\n",
    "\n",
    "* [**Glue Workflow**](https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html) - Orchestration engine for ML workflow.\n",
    "* [**Glue Job**](https://docs.aws.amazon.com/glue/latest/dg/author-job.html) - Business logic for ETL or python shell.\n",
    "* [**Glue Trigger**](https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html) - Triggers Glue Job as steps.\n",
    "\n",
    "Once the Glue Workflow is created, you may view the the detail via: AWS Glue Console / Workflow / (To select the created workflow). It should be similar like:\n",
    "\n",
    "<div align=\"center\"><img width=500 src=\"images/glue_workflow.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AWS Glue Workflow\n",
    "\n",
    "#### Create Glue Workflow Object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client(\"glue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_workflow_name = f\"CustomerChurnMLWorkflow-{id}\"\n",
    "response = glue_client.create_workflow(\n",
    "    Name=glue_workflow_name,\n",
    "    Description='AWS Glue workflow to process data and create training jobs'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Job\n",
    "data_processing_script_path = S3Uploader.upload(\n",
    "    local_path=\"./code/glue_preprocessing.py\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/glue/scripts\",\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "data_processing_job_name = f\"DataProcessingJob-{id}\"\n",
    "response = glue_client.create_job(\n",
    "    Name=data_processing_job_name,\n",
    "    Description='Preparing data for SageMaker training',\n",
    "    Role=glue_role_arn,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': data_processing_script_path,\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        \"--job-bookmark-option\": \"job-bookmark-enable\",\n",
    "        \"--enable-metrics\": \"\",\n",
    "        \"--additional-python-modules\": \"pyarrow==2,awswrangler==2.9.0,fsspec==0.7.4\"\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    Timeout=60,\n",
    "    MaxCapacity=10.0,\n",
    "    GlueVersion='2.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training & Deployment Job\n",
    "model_training_deployment_script_path = S3Uploader.upload(\n",
    "    local_path=\"./code/model_training_deployment.py\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/{prefix}/glue/scripts\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "model_training_deployment_job_name = f\"ModelTrainingDeploymentJob-{id}\"\n",
    "response = glue_client.create_job(\n",
    "    Name=model_training_deployment_job_name,\n",
    "    Description='Model training and deployment',\n",
    "    Role=glue_role_arn,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'pythonshell',\n",
    "        'ScriptLocation': model_training_deployment_script_path,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        \"--job-bookmark-option\": \"job-bookmark-enable\",\n",
    "        \"--enable-metrics\": \"\"\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    Timeout=60,\n",
    "    MaxCapacity=1,\n",
    "    GlueVersion='1.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = f\"s3://{bucket}/{prefix}/output\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    ")\n",
    "\n",
    "processed_data, sagemaker_execution_role, image_uri, model_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processing_trigger_name = f'TriggerDataProcessingJob-{id}'\n",
    "response = glue_client.create_trigger(\n",
    "    Name=data_processing_trigger_name,\n",
    "    Description='Triggering Data Processing Job',\n",
    "    Type='ON_DEMAND',\n",
    "    WorkflowName=glue_workflow_name,\n",
    "    Actions=[\n",
    "        {\n",
    "            'JobName': data_processing_job_name,\n",
    "            'Arguments': {\n",
    "                '--INPUT_DIR': raw_data,\n",
    "                '--PROCESSED_DIR': processed_data\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_deploy_trigger_name = f'TriggerModelTrainingDeploymentJob-{id}'\n",
    "response = glue_client.create_trigger(\n",
    "    Name=model_train_deploy_trigger_name,\n",
    "    Description='Triggering Model Training Deployment Job',\n",
    "    WorkflowName=glue_workflow_name,\n",
    "    Type='CONDITIONAL',\n",
    "    StartOnCreation=True,\n",
    "    Predicate={\n",
    "        'Conditions': [\n",
    "            {\n",
    "                'LogicalOperator': 'EQUALS',\n",
    "                'JobName': data_processing_job_name,\n",
    "                'State': 'SUCCEEDED'\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    Actions=[\n",
    "        {\n",
    "            'JobName': model_training_deployment_job_name,\n",
    "            'Arguments': {\n",
    "                '--train_input_path': processed_data,\n",
    "                '--model_output_path': model_output_path,\n",
    "                '--algorithm_image': image_uri,\n",
    "                '--role_arn': sagemaker_execution_role,\n",
    "                '--endpoint_name': endpoint_name\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "from datetime import datetime\n",
    "# todo\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "\n",
    "class ModelRun:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        # todo\n",
    "        # args = getResolvedOptions(sys.argv, ['train_input_path', 'model_output_path', 'algorithm_image', 'role_arn', 'endpoint_name'])\n",
    "        current_time = datetime.now()\n",
    "        self.train_input_path = args['train_input_path']\n",
    "        self.model_output_path = args['model_output_path']\n",
    "        self.algorithm_image = args['algorithm_image']\n",
    "        self.role_arn = args['role_arn']\n",
    "        timestamp_suffix = str(current_time.month) + \"-\" + str(current_time.day) + \"-\" + str(current_time.hour) + \"-\" + str(current_time.minute)\n",
    "        self.training_job_name = 'gw-xgb-churn-pred' + timestamp_suffix\n",
    "        self.endpoint = args['endpoint_name']\n",
    "        \n",
    "    def create_training_job(self):\n",
    "        print(\"Started training job...\")\n",
    "        \n",
    "        try:\n",
    "            response = sagemaker.create_training_job(\n",
    "                TrainingJobName=self.training_job_name,\n",
    "                HyperParameters={\n",
    "                    'max_depth': '5',\n",
    "                    'eta': '0.2',\n",
    "                    'gamma': '4',\n",
    "                    'min_child_weight': '6',\n",
    "                    'subsample': '0.8',\n",
    "                    'silent': '0',\n",
    "                    'objective': 'binary:logistic',\n",
    "                    'num_round': '100',\n",
    "                    'eval_metric': 'auc'\n",
    "                },\n",
    "                AlgorithmSpecification={\n",
    "                    'TrainingImage': self.algorithm_image,\n",
    "                    'TrainingInputMode': 'File'\n",
    "                },\n",
    "                RoleArn=self.role_arn,\n",
    "                InputDataConfig=[\n",
    "                    {\n",
    "                        'ChannelName': 'train',\n",
    "                        'DataSource': {\n",
    "                            'S3DataSource': {\n",
    "                                'S3DataType': 'S3Prefix',\n",
    "                                'S3Uri': self.train_input_path + '/train',\n",
    "                                'S3DataDistributionType': 'FullyReplicated'\n",
    "                            }\n",
    "                        },\n",
    "                        'ContentType': 'text/csv',\n",
    "                        'CompressionType': 'None'\n",
    "                    },\n",
    "                    {\n",
    "                        'ChannelName': 'validation',\n",
    "                        'DataSource': {\n",
    "                            'S3DataSource': {\n",
    "                                'S3DataType': 'S3Prefix',\n",
    "                                'S3Uri': self.train_input_path + '/validation',\n",
    "                                'S3DataDistributionType': 'FullyReplicated'\n",
    "                            }\n",
    "                        },\n",
    "                        'ContentType': 'text/csv',\n",
    "                        'CompressionType': 'None'\n",
    "                    }\n",
    "                ],\n",
    "                OutputDataConfig={\n",
    "                    'S3OutputPath': self.model_output_path\n",
    "                },\n",
    "                ResourceConfig={\n",
    "                    'InstanceType': 'ml.m5.xlarge',\n",
    "                    'InstanceCount': 1,\n",
    "                    'VolumeSizeInGB': 20\n",
    "                },\n",
    "                StoppingCondition={\n",
    "                    'MaxRuntimeInSeconds': 86400\n",
    "                }\n",
    "            )\n",
    "            print(\"Training job has been created...\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Unable to create training job')\n",
    "            raise(e)\n",
    "            \n",
    "    def describe_training_job(self):\n",
    "        status = sagemaker.describe_training_job(\n",
    "            TrainingJobName=self.training_job_name\n",
    "        )\n",
    "        print(self.training_job_name + \" job status: \", status)\n",
    "        print(\"Waiting for \" + self.training_job_name + \" training job to complete...\")\n",
    "        sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=self.training_job_name)\n",
    "        resp = sagemaker.describe_training_job(TrainingJobName=self.training_job_name)\n",
    "        status = resp['TrainingJobStatus']\n",
    "        print(\"Training job \" + self.training_job_name + \" ended with status: \" + status)\n",
    "        if status == 'Failed':\n",
    "            message = resp['FailureReason']\n",
    "            print('Training job {} failed with the following error: {}'.format(self.training_job_name, message))\n",
    "            raise Exception('Creation of sagemaker Training job failed')\n",
    "        return status\n",
    "\n",
    "    def create_endpoint_config(self):\n",
    "\n",
    "        endpoint_name = self.endpoint\n",
    "\n",
    "        print(\"Creating model..\")\n",
    "        create_model = sagemaker.create_model(\n",
    "            ModelName=endpoint_name,\n",
    "            PrimaryContainer=\n",
    "            {\n",
    "                'Image': self.algorithm_image,\n",
    "                'ModelDataUrl': f\"{self.model_output_path}/{self.training_job_name}/output/model.tar.gz\"\n",
    "            },\n",
    "            ExecutionRoleArn=self.role_arn\n",
    "        )\n",
    "\n",
    "        resp = sagemaker.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_name,\n",
    "            ProductionVariants=[\n",
    "                {\n",
    "                    'VariantName': '{}-variant-1'.format(endpoint_name),\n",
    "                    'ModelName': endpoint_name,\n",
    "                    'InitialInstanceCount': 1,\n",
    "                    'InstanceType': 'ml.m5.large'\n",
    "                }\n",
    "            ])\n",
    "\n",
    "        print(resp)\n",
    "        return resp\n",
    "\n",
    "    def create_endpoint(self):\n",
    "        print(\"Creating endpoint..\")\n",
    "        endpoint_name = self.endpoint\n",
    "        response = sagemaker.create_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            EndpointConfigName=endpoint_name\n",
    "        )\n",
    "        print(response)\n",
    "\n",
    "    def describe_endpoint(self):\n",
    "        status = sagemaker.describe_endpoint(EndpointName=self.endpoint)['EndpointStatus']\n",
    "        print(self.endpoint + \"endpoint is now in status:\", status)\n",
    "        print(\"Waiting for \" + self.endpoint + \" to be In-service...\")\n",
    "        sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=self.endpoint)\n",
    "        resp = sagemaker.describe_endpoint(EndpointName=self.endpoint)\n",
    "        status = resp['EndpointStatus']\n",
    "        print(self.endpoint + \" endpoint is now in status:\", status)\n",
    "        if status == 'Failed':\n",
    "            message = resp['FailureReason']\n",
    "            print('Test Endpoint {} creation failed with the following error: {}'.format(self.endpoint, message))\n",
    "            raise Exception('Endpoint creation failed')\n",
    "        return status\n",
    "\n",
    "    \n",
    "    def create_batch_transform_job(self):\n",
    "        batch_job_name = self.batch_transform_job_name\n",
    "        model_name = self.model_name\n",
    "        inference_output_location = self.inference_output_location\n",
    "        inference_input_location = self.inference_input_location\n",
    "        \n",
    "        request = {\n",
    "            \"TransformJobName\": batch_job_name,\n",
    "            \"ModelName\": model_name,\n",
    "            \"TransformOutput\": {\n",
    "                \"S3OutputPath\": inference_output_location,\n",
    "                \"Accept\": \"text/csv\",\n",
    "                \"AssembleWith\": \"Line\",\n",
    "            },\n",
    "            \"TransformInput\": {\n",
    "                \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": inference_input_location}},\n",
    "                \"ContentType\": \"text/csv\",\n",
    "                \"SplitType\": \"Line\",\n",
    "                \"CompressionType\": \"None\",\n",
    "            },\n",
    "            \"TransformResources\": {\"InstanceType\": \"ml.m5.xlarge\", \"InstanceCount\": 1},\n",
    "        }\n",
    "        sagemaker.create_transform_job(**request)\n",
    "        print(\"Created Transform job with name: \", batch_job_name)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SDK to sagemaker\n",
    "sagemaker = boto3.client('sagemaker')    \n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "args = dict(\n",
    "    train_input_path=processed_data,\n",
    "    model_output_path=model_output_path,\n",
    "    algorithm_image=image_uri,\n",
    "    role_arn=sagemaker_execution_role,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "obj = ModelRun(args)\n",
    "\n",
    "# Create training job\n",
    "obj.create_training_job()\n",
    "\n",
    "# Describe training job\n",
    "status = obj.describe_training_job()\n",
    "\n",
    "# Create endpoint conf\n",
    "resp = obj.create_endpoint_config()\n",
    "\n",
    "# Create endpoint for model\n",
    "obj.create_endpoint()\n",
    "\n",
    "# Describe endpoint\n",
    "status = obj.describe_endpoint()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your workflow using the workflow definition above, and render the graph with [render_graph](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.render_graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "response = glue_client.start_workflow_run(\n",
    "    Name=glue_workflow_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_workflow_state(workflow_name, run_id):\n",
    "    resp = glue_client.get_workflow_run(\n",
    "        Name=workflow_name,\n",
    "        RunId=run_id,\n",
    "        IncludeGraph=True\n",
    "    )\n",
    "    return resp['Run']['Status']\n",
    "\n",
    "print('Checking workflow state:')\n",
    "while True:\n",
    "    workflow_status = check_workflow_state(glue_workflow_name, response['RunId'])\n",
    "    if workflow_status in ['COMPLETED', 'STOPPED', 'ERROR']:\n",
    "        print(workflow_status)\n",
    "        break\n",
    "    else:\n",
    "        print('.')\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the deployed model\n",
    "\n",
    "Once the workflow execution is completed, we can execute below code cells to evaluate the test data result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "sagemaker_runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "sagemaker_client = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'gw-customer-churn-endpoint-8-3-6-51'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(endpoint_name, s3_object_uri):\n",
    "    # download the data\n",
    "    df = pd.read_csv(s3_object_uri, header=None)\n",
    "    \n",
    "    payload = df[df.columns[1:]].to_csv(header=False, index=False).encode(\"utf-8\")\n",
    "\n",
    "    response = sagemaker_runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType='text/csv', \n",
    "        Body=payload)\n",
    "\n",
    "    result = response['Body'].read().decode()\n",
    "\n",
    "    prediction_probabilities = np.asarray(result.split(','), dtype=float)\n",
    "    predictions = np.round(prediction_probabilities)\n",
    "    \n",
    "    y_test = df[0]\n",
    "\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    fpr, tpr, _ = roc_curve(y_test, prediction_probabilities)\n",
    "    \n",
    "    return accuracy, precision, recall, conf_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_uri = f\"{test_data}{test_data_file}\"\n",
    "accuracy, precision, recall, conf_matrix = evaluate_model(endpoint_name, test_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96,\n",
       " 0.9615384615384616,\n",
       " 0.9615384615384616,\n",
       " array([[23,  1],\n",
       "        [ 1, 25]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, precision, recall, conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_runtime_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_package_group(model_package_group_name):\n",
    "    try:\n",
    "        return sagemaker_client.describe_model_package_group(\n",
    "            ModelPackageGroupName=model_package_group_name\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # to create model package group\n",
    "        model_package_group_input_dict = {\n",
    "         \"ModelPackageGroupName\" : model_package_group_name,\n",
    "         \"ModelPackageGroupDescription\" : \"Sample model package group from Glue Workflow Demo\"\n",
    "        }\n",
    "\n",
    "        response = sagemaker_client.create_model_package_group(**model_package_group_input_dict)\n",
    "        return response\n",
    "    \n",
    "def create_model_package_group_version(model_package_group_name, container):\n",
    "    modelpackage_inference_specification =  {\n",
    "        \"InferenceSpecification\": {\n",
    "            \"Containers\": [\n",
    "             {\n",
    "                \"Image\": container['Image'],\n",
    "                \"ModelDataUrl\": container['ModelDataUrl']\n",
    "             }\n",
    "            ],\n",
    "            \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "            \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    create_model_package_input_dict = {\n",
    "        \"ModelPackageGroupName\" : model_package_group_name,\n",
    "        \"ModelPackageDescription\" : \"Model to predict customer churn.\",\n",
    "        \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "    }\n",
    "    create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "\n",
    "    create_model_package_response = sagemaker_client.create_model_package(**create_model_package_input_dict)\n",
    "    return create_model_package_response\n",
    "\n",
    "def get_model_name(endpoint_name):\n",
    "    response = sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    return response['ProductionVariants'][0]['ModelName']    \n",
    "\n",
    "def register_model_package_group(endpoint_name):\n",
    "    \n",
    "    model_name = get_model_name(endpoint_name)\n",
    "    \n",
    "    response = sagemaker_client.describe_model(ModelName = model_name)\n",
    "    container = response['PrimaryContainer']\n",
    "    \n",
    "    create_model_package_group(model_name)\n",
    "    create_model_package_group_version(model_name, container)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model group\n",
    "register_model_package_group(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_transform_job(endpoint_name, inference_input_location, inference_output_location):\n",
    "    model_name = get_model_name(endpoint_name)\n",
    "    batch_job_name = f\"batch-transform-job-by-{model_name}\"\n",
    "    inference_output_location = inference_output_location\n",
    "    inference_input_location = inference_input_location\n",
    "\n",
    "    request = {\n",
    "        \"TransformJobName\": batch_job_name,\n",
    "        \"ModelName\": model_name,\n",
    "        \"TransformOutput\": {\n",
    "            \"S3OutputPath\": inference_output_location,\n",
    "            \"Accept\": \"text/csv\",\n",
    "            \"AssembleWith\": \"Line\",\n",
    "        },\n",
    "        \"TransformInput\": {\n",
    "            \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": inference_input_location}},\n",
    "            \"ContentType\": \"text/csv\",\n",
    "            \"SplitType\": \"Line\",\n",
    "            \"CompressionType\": \"None\",\n",
    "        },\n",
    "        \"TransformResources\": {\"InstanceType\": \"ml.m5.xlarge\", \"InstanceCount\": 1},\n",
    "    }\n",
    "    sagemaker.create_transform_job(**request)\n",
    "    print(\"Created Transform job with name: \", batch_job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Transform job with name:  batch-transform-job-by-gw-customer-churn-endpoint-8-3-6-51\n"
     ]
    }
   ],
   "source": [
    "# kick off batch transform job\n",
    "inference_output_location = batch_transform_output\n",
    "inference_input_location = test_data_uri\n",
    "create_batch_transform_job(endpoint_name, inference_input_location, inference_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, make sure to clean up your AWS account by deleting resources you won't be reusing. Uncomment the code below and run the cell to delete the Glue job, Lambda function, and Step Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete the jobs\n",
    "for job_name in [data_processing_job_name, model_training_deployment_job_name]:\n",
    "    glue_client.delete_job(JobName=job_name)\n",
    "\n",
    "# delete the triggers    \n",
    "for trigger_name in [data_processing_trigger_name, model_train_deploy_trigger_name]:\n",
    "    glue_client.delete_trigger(Name=trigger_name)\n",
    "    \n",
    "# deletion\n",
    "response = glue_client.delete_workflow(\n",
    "    Name=glue_workflow_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.Session().client('sagemaker')\n",
    "\n",
    "sagemaker_client.delete_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
